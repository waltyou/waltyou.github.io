---
layout: post
title: Java 并发编程实战-学习日志（三）2：性能与可伸缩性
date: 2019-10-30 18:01:04
author: admin
comments: true
categories: [Java]
tags: [Java, Concurrency, Java Concurrency In Practice]
---

线程的最主要目的是提高程序的运行性能，使程序更加充分地发挥系统的可用处理能力，从而提高系统的资源利用率，此外，还可以使程序在运行现有任务的情况下立即开始处理新的任务，从而提高系统的响应性。许多提升性能同样会增加复杂性，也会增加在安全性和活跃性上发生失败的风险。

<!-- more -->

---

* 目录
{:toc}
---

# 对性能的思考

对于一个给定的操作，通常会缺乏某种特定的资源，例如 CPU 时钟周期、内存、网络带宽、I/O 带宽。数据库请求、磁盘空间以及其他资源。当操作性能由于某种特定的资源而受到限制时，通常将该操作称为资源密集型的操作，例如，CPU 密集型、数据库密集型等。

通过并发来获得更好的性能：更有效地利用现有处理资源，以及在出现新的处理资源时使程序尽可能地利用这些新资源。从性能监视的视角看，CPU 需要尽可能保持忙碌状态。

## 1. 性能与可伸缩性

应用程序的性能可以采用多个指标来衡量，例如服务时间、延迟时间、吞吐率、效率、可伸缩性以及容量等。其中一些指标（服务时间、等待时间）用于衡量程序的 "运行速度"，即某个指定的任务单元需要 "多快" 才能处理完成。另一些指标（生产量、吞吐量）用于程序的 "处理能力"，即在计算资源一定的情况下，能完成 "多少" 工作。

可伸缩性指的是：当增加计算资源时（例如 CPU、内存、存储容量或 I/O 带宽），程序的吞吐量或者处理能力能相应地增加。

## 2. 评估各种性能权衡因素：

在几乎所有的工程决策中都会涉及某些形式的权衡。在作出正确的权衡时通常会缺少相应的信息，例如，"快速排序" 算法在大规模数据集上的执行效率非常高，但对于小规模的数据集，"冒泡排序" 实际上更高效。要实现一个高效的排序算法，那么需要知道被处理数据集的大小，还有衡量优化的指标，包括：平均计算时间、最差时间、可预知性等。大多数优化措施都不成熟的原因之一是通常无法获得一组明确的需求。

进行决策时，会通过增加某种形式的成本来降低另一种形式的开销（例如，增加内存使用量以降低服务时间），也会通过增加开销来换取安全性。很多性能优化措施通常都是以牺牲可读性或可维护性，或会破坏面向对象的设计原则，例如需要打破封装，或带来更高的错误风险，因为通常越快的算法就越复杂。

对性能的提升可能使并发错误的最大来源，例如采用双重检查锁来减少同步的使用，由于并发错误是最难追踪和消除的错误，因此对于任何可能会引入这类错误的措施，都需要谨慎实施。

对性能调优时，一定要有明确的性能需求（才能知道什么时候需要调优，以及什么时候应该停止），此外还需要一个测试程序以及真实的配置和负载等环境。在对性能调优后，需要再次测量以验证是否到达了预期的性能提升目标。在许多优化措施中带来的安全性和可维护性等风险非常高。免费的 perfbar 应用程序可以给出 CPU 的忙碌程度信息，而通常目标就是使 CPU 保持忙碌状态，因此可以有效地评估是否需要进行性能调优或者已实现的调优效果如何。



#  Amdahl 定律

如果可用资源越多，那么问题的解决速度就越快，例如，如果参与收割庄稼的工人越多，那么就能越快地完成工作。而有些任务本质上是串行的，例如，即使增加再多的工人也不可能增加作物的生长速度。如果使用线程主要是为了发挥多个处理器的处理能力，那么就必须对问题进行合理的并行分解，并使得程序能有效地使用这种潜在的并行能力。

​    大多数并发程序都是由一系列的并行工作和串行工作组成。Amdahl 定律描述的：在增加计算资源的情况下，程序在理论上能够实现最高加速比，这个值取决于程序中可并行组件与串行组件所占比重。假定 F 是必须被串行执行的部分，根据 Amdahl 定律，在包含 N 个处理器的机器中，最高的加速比：

​    **Speedup <= 1 /  (F + (1-F) / N)**

当 N 趋近于无穷大时，最大的加速比趋近于 1/F。因此，如果程序有 50% 的计算需要串行执行，那么最高的加速比只能是 2（不管有多少个线程可用）；如果在初中有 10% 的计算需要串行执行，那么最高的加速比将接近 10。Amdahl 定律还量化了串行化的效率开销。在拥有 10 个处理器的系统中，如果程序中有 10% 的部分需要串行化执行，那么最高的加速比为 5.3（53%的使用率），在拥有 100 个处理器的系统中，加速比可达到 9.2（9%的使用率）。即使拥有无限多的 CPU，加速比也不可能为 10。随着处理器的增加，即使串行部分所占的百分比很小，也会极大地限制当增加计算资源时能够提升的吞吐率。




[![](/images/posts/java-Amdahl.gif)](/images/posts/java-Amdahl.gif) 



## 1. 示例：在各种框架中隐藏的串行部分

串行部分是如何隐藏在应用程序的架构中，可以比较当增加线程时吞吐量的变化，并根据观察到的可伸缩性变化来推断串行部分中的差异。例如多个线程反复地从一个共享 Queue 中取出元素进行处理，尽管每次运行都表示相同的工作量，但改变队列的实现方式，就能对可伸缩性产生明显的影响。

ConcurrentLinkedQueue 的吞吐量不断提升，直到到达了处理器数量上限，之后将基本保持不变。另一方面，当线程数量小于 3 时，同步 LinkedList 的吞吐量也会有某种程度的提升，但是之后会由于同步开销的增加而下降。当线程数量达到 4 或 5 个时，竞争将非常激烈，甚至每次访问队列都会在锁上发生竞争，此时的吞吐量主要受到上下文切换的限制。

吞吐量的差异来源于两个队列中不同比例的串行部分。同步 LinkedList 采用单个锁来保护整个队列的状态，并且在 offer 和 remove 等方法的调用期间都将持有这个锁。ConcurrentLinkedQueue 使用了一种更复杂的非阻塞队列算法，该算法使用原子引用来更新各个链接指针。在第一个队列中，整个的插入或删除操作都将串行执行，而在第二个队列中，只有对指针的更新操作需要串行执行。



# 线程引入的开销

单线程程序既不存在线程调度，也不存在同步开销，而且不需要使用锁来保证数据结构的一致性。在多个线程的调度和协调过程中都需要一定的性能开销：对于为了提升性能而引入的线程来说，并行带来的性能提升必须超过并发导致的开销。

## 1. 上下文切换

如果主线程是唯一的线程，它基本上不会被调度出去。另一方面，如果可运行的线程数大于 CPU 的数量，那么操作系统最终会将某个正在运行的线程调度出来，从而使其他线程能够使用 CPU。这将导致一次上下文切换，在这个过程中将保存当前运行线程的执行上下文，并将新调度进来的线程的执行上下文设置为当前上下文。

切换上下文需要一定的开销，而在线程调度过程中需要访问由操作系统和 JVM 共享的数据结构。应用程序、操作系统以及 JVM 都使用一组相同的 CPU。在 JVM 和操作系统的代码中消耗越多的 CPU 时钟周期，应用程序的可用 CPU 时钟周期就越少。但上下文切换的开销并不只是包含 JVM 和操作系统的开销。当一个新的线程被切换进来时，它所需要的数据可能不在当前处理器的本地缓存中，因此上下文切换将导致一些缓存缺失，因而线程在首次调度运行时会更加缓慢。这就是调度器会为每个可运行的线程分配一个最小执行时间，即使有许多其他的线程正在等待执行：它将上下文切换的开销分摊到更多不会中断的执行时间上，从而提高整体的吞吐量。（以损失响应性为代价）。

当线程由于等待某个发生竞争的锁而被阻塞时，JVM 通常会将这个线程挂起，并允许它被交换出去。如果线程频繁发生阻塞，那么它们将无法使用完整的调度时间片。在程序中发生越多阻塞（包括 I/O 阻塞，等待获取发生竞争的锁，或者在条件变量上等待），与 CPU 密集型的程序就会发生越多的上下文切换，从而增加调度开销，并降低吞吐量。

UNIX 系统的 vmstat 命令和 Windows 系统的 perfmon 工具都能报告上下文切换次数以及在内核中执行时间所占比例等信息。如果内核占用率较高（超过 10%），那么通常表示调度活动发生得很频繁，这很有可能是由 I/O 或竞争锁导致的阻塞引起的。

## 2. 内存同步 

在 synchronized 和 volatile 提供的可见性保证中可能会使用一些特殊指令，即内存栅栏（Memory Barrier）。内存栅栏可以刷新缓存，使缓存无效，刷新硬件的写缓冲，以及停止执行管道。内存栅栏可能同样会对性能带来间接的影响，因为它们将抑制一些编译器优化操作。在内存栅栏中，大多数操作都是不能被重排序的。

在评估同步操作带来的性能影响时，区分由竞争的同步和无竞争的同步非常重要。synchronized 机制针对无竞争的同步进行了优化（volatile通常是非竞争的）。虽然无竞争同步的开销不为零，但它对应用程序整体性能的影响微乎其微，而另一种方法不仅会破坏安全性，而且会使维护人员经历痛苦的排错过程。

现代的 JVM 能通过优化来去掉一些不会发生竞争的锁，从而减少不必要的同步开销。如果一个锁对象只能由当前线程访问，那么 JVM 就可以通过优化来去掉这个锁获取操作，因为另一个线程无法与当前线程在这个锁上发生同步。一些更完备的 JVM 能通过逸出分析（Escape Analysis）来找出不会发布到堆的本地对象引用（因此这个引用是线程本地的）。

```java
public String getStoogeNames(){
  List<String> stooges = new Vector<String>();
  stooges.add("1");
  stooges.add("2");
  stooges.add("3");
  return stooges.toString();
}
```

对 List 的唯一引用就是局部变量 stooges，并且所有封闭在栈中的变量都会自动成为线程本地变量。在 getStoogeNames 执行过程中，至少会将 Vector 上的锁获取/释放 4 次，每次调用 add 或 toString 时都会执行 1 次。然而，一个智能的运行时编译器通常会分析这些调用，从而使 stooges 及其内部状态不会逸出，因此可以去掉这 4 次对锁获取的操作。即使不进行逸出分析，编译器也可以执行锁粒度粗化（Lock Coarsening）操作，即将邻近的同步代码块用同一个锁合并起来。在 getStoogeNames 中，如果 JVM 进行锁粒度粗化，可能会把 3 个 add 与 1 个 toString 调用合并为单个锁获取/释放操作，并采用启发式方法来评估同步代码块中采用同步操作以及指令之间的相对开销。这不仅减少了同步的开销，同时还能使优化器处理更大的代码块，从而可能实现进一步的优化。

某个线程中的同步可能会影响其他线程的性能。同步会增加共享内存总线上的通信量，总线的带宽是有限的，并且所有的处理器都将共享这条总线。如果有多个线程竞争同步带宽，那么所有使用了同步的线程都会受到影响。

## 3. 阻塞

非竞争的同步可以完全在 JVM 中进行处理，而竞争的同步可能需要操作系统的介入，从而增加开销。当在锁上发生竞争时，竞争失败的线程肯定会阻塞。JVM 在实现阻塞行为时，可以采用**自旋等待**（Spin-Watiing，指通过循环不断尝试获取锁直到成功）或者通过操作系统挂起被**阻塞**的线程。这两种方式的效率高低，取决于上下文切换的开销以及在成功获取锁之前需要等待的时间。如果等待时间较短，则适合采用自旋等待方式，而如果等待时间较长，则适合采用线程挂起方式。有些 JVM 将根据对历史等待时间的分析数据在这两者之间进行选择，但大多数 JVM 在等待锁时都只是将线程挂起。

当线程无法获取某个锁或者由于在某个条件等待或在 I/O 操作上阻塞时，需要被挂起，在这个过程中将包含两次额外的上下文切换，以及所有必要的操作系统操作和缓存操作：被阻塞的线程在其执行时间片还未用完之前就被交换出去，而在随后当要获取的锁或者其他资源可用时，又再次被切换回来。（由于锁竞争而导致阻塞时，线程在持有锁时将存在一定的开销：当它释放锁时，必须要告诉操作系统恢复运行阻塞的线程）







## 未完待续。。。