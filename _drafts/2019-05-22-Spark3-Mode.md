---
layout: post
title: Spark 中的三种 mode
date: 2019-05-22 21:48:04
author: admin
comments: true
categories: [Spark]
tags: [Spark, Big Data]
---


<!-- more -->

* 目录
{:toc}
---

# Cluster Mode

In cluster mode, A user submits a pre-complied JAR, Python Script or R script to the cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. The cluster manager is responsible for maintaining all Spark Application related processes. 

# Client Mode

Client mode is almost the same as cluster mode, however the Spark driver remains on the client machine that submitted the application. This means that the client machine is responsible for maintaining the Spark driver process and the cluster manager maintains the executor processes.

# Local Mode

Local mode is very different from both cluster and client modes. Local mode runs the entire Spark Application on a single machine and it achieves parallelism through threads on that single machine. We use this mode as a way to learn Spark, test applications or just experiment with Spark. Don’t use this mode for running applications that you deploy to production.

# The Life cycle of a Spark Application

## outside

1. submit 代码到集群中
2. cluster manager 创建driver
3. driver 中 创建 sparksession or sparkcontext， 用来和 CM 交流，让它在集群中分配executor
4. driver 和 workers 互相交流，进行计算和移动数据
5. worker 们根据 driver 的安排来执行 task，并向 driver 报告运行结果
6. application 成功后，driver 退出。 CM 释放资源。

## Inside

1. Creating a SparkSession
2. action 触发生成一个 job ，并构建 DAG 图
3. 划分 DAG 为 stage， 划分 stage 为 tasks
4. 运行 tasks











[How does Apache Spark run on a cluster?](https://towardsdatascience.com/how-does-apache-spark-run-on-a-cluster-974ec2731f20)